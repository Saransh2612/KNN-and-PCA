{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNTscjySvI3hfA/5G1KALmE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Question 1: What is K-Nearest Neighbors (KNN) and how does it work in both\n","classification and regression problems?\n","-- K-Nearest Neighbors (KNN) is a simple, non-parametric, supervised learning algorithm used for both classification and regression problems.\n","\n","How KNN Works (Basic Idea):\n","\n","KNN makes predictions based on the similarity (distance) between the input data point and its K nearest neighbors in the training dataset.\n","\n","\"K\" is the number of nearest neighbors considered.\n","\n","The similarity is usually measured by Euclidean distance, though other metrics like Manhattan or Minkowski distances can also be used.\n","\n","KNN for Classification:\n","\n","Input: A new, unlabeled data point.\n","\n","Process:\n","\n","Calculate the distance between the new point and all training points.\n","\n","Identify the K closest neighbors.\n","\n","Count the most frequent class among those neighbors.\n","\n","Output: Assign the majority class to the new data point.\n","\n","Example:\n","If\n","ùêæ\n","=\n","5\n","K=5 and among the 5 nearest neighbors:\n","\n","3 are class A\n","\n","2 are class B\n","Then the prediction is Class A.\n","\n","KNN for Regression:\n","\n","Input: A new data point without a target value.\n","\n","Process:\n","\n","Calculate the distance to all training points.\n","\n","Find the K nearest neighbors.\n","\n","Take the average (or weighted average) of their target values.\n","\n","Output: Return the mean (or weighted mean) as the predicted value.\n","\n","Example:\n","If the nearest 3 neighbors have values [5.2, 6.1, 5.7], the predicted value would be:\n","\n","5.2\n","+\n","6.1\n","+\n","5.7\n","3\n","=\n","5.67\n","3\n","5.2+6.1+5.7\n","\t‚Äã\n","\n","=5.67\n","\n","ey Considerations:\n","\n","Choice of K:\n","\n","Too small ‚Üí model becomes sensitive to noise (overfitting).\n","\n","Too large ‚Üí model may lose important local patterns (underfitting).\n","\n","Feature scaling: KNN is distance-based, so it is sensitive to the scale of features. Normalization or standardization is usually needed.\n","\n","Lazy learning: KNN does no learning during training; it simply stores the training data and does all computation during prediction.\n","\n","Computational cost: Prediction can be slow on large datasets because it must compute distances to all training points.\n","\n","| Feature       | KNN for Classification       | KNN for Regression         |\n","| ------------- | ---------------------------- | -------------------------- |\n","| Output        | Most common class (majority) | Average of neighbor values |\n","| Decision Rule | Voting                       | Averaging                  |\n","| Use Case      | Label prediction             | Value prediction           |\n"],"metadata":{"id":"3xhNa9xzbwtR"}},{"cell_type":"markdown","source":["Question 2: What is the Curse of Dimensionality and how does it affect KNN\n","performance?\n","-- What is the Curse of Dimensionality?\n","\n","The Curse of Dimensionality refers to various problems that arise when analyzing and organizing data in high-dimensional spaces (i.e., when the number of features or dimensions is very large).\n","\n","As dimensions increase:\n","\n","Data becomes sparse.\n","\n","Distance metrics (like Euclidean distance) become less meaningful.\n","\n","The volume of the space increases exponentially, so data points become more spread out.\n","\n","All points tend to look equally far apart, making it hard to distinguish neighbors.\n","\n"," How It Affects KNN Performance:\n","\n","Since KNN relies on distance calculations, the curse of dimensionality causes major issues for its accuracy and efficiency in high-dimensional data:\n","\n"," 1. Distance Becomes Less Informative\n","\n","In high dimensions, the difference between the nearest and farthest neighbors shrinks.\n","\n","This reduces the contrast between close and far points.\n","\n","As a result, KNN may struggle to find meaningful \"nearest\" neighbors.\n","\n"," Example: In 100 dimensions, the distance between all points tends to converge, making it difficult for KNN to distinguish which neighbors are truly close.\n","\n"," 2. Increased Sparsity\n","\n","The dataset becomes sparser as dimensionality increases, even if the number of samples remains the same.\n","\n","Sparse data makes it difficult to form reliable neighborhoods for prediction.\n","\n"," 3. Higher Computational Cost\n","\n","Distance computation becomes expensive in high dimensions.\n","\n","Since KNN requires computing distance to all training points, prediction time increases dramatically.\n","\n"," 4. Risk of Overfitting\n","\n","High-dimensional data often contains irrelevant or noisy features.\n","\n","KNN treats all features equally unless feature selection or weighting is applied.\n","\n","This can cause KNN to overfit on noise, leading to poor generalization.\n","\n","Curse of Dimensionality: High-dimensional data weakens the effectiveness of distance-based methods like KNN.\n","\n","It causes distance distortion, data sparsity, computational inefficiency, and overfitting.\n","\n","To improve KNN performance in high dimensions, reduce dimensionality and focus on relevant features."],"metadata":{"id":"HZSp7tSSbwcq"}},{"cell_type":"markdown","source":["Question 3: What is Principal Component Analysis (PCA)? How is it different from\n","feature selection?\n","--  What is Principal Component Analysis (PCA)?\n","\n","PCA is a dimensionality reduction technique used to reduce the number of features in a dataset while retaining as much variance (information) as possible.\n","\n","It does this by transforming the original features into a new set of uncorrelated features called principal components.\n","\n"," How PCA Works:\n","\n","Standardize the data (mean = 0, variance = 1).\n","\n","Compute the covariance matrix of the features.\n","\n","Compute eigenvalues and eigenvectors of the covariance matrix.\n","\n","Select the top K eigenvectors (those with the largest eigenvalues).\n","\n","Project the data onto these top eigenvectors (principal components).\n","\n","Each principal component is a linear combination of the original features, and they are ordered by the amount of variance they capture.\n","\n"," Example:\n","\n","Suppose you have 3 features: height, weight, age.\n","\n","PCA might create:\n","\n","PC1: 0.6√óheight + 0.7√óweight + 0.2√óage\n","\n","PC2: -0.5√óheight + 0.1√óweight + 0.9√óage\n","\n","You can keep just the top 1 or 2 principal components instead of all 3 original features.\n","\n"," PCA Is Used To:\n","\n","Reduce dimensionality\n","\n","Remove multicollinearity\n","\n","Speed up training\n","\n","Visualize high-dimensional data (e.g., 2D plot of PC1 vs PC2)\n","\n","| Aspect               | PCA (Feature Extraction)                    | Feature Selection                     |\n","| -------------------- | ------------------------------------------- | ------------------------------------- |\n","| **What it does**     | Creates new features (principal components) | Chooses a subset of existing features |\n","| **Output Features**  | Transformed (linear combinations)           | Original, unmodified features         |\n","| **Purpose**          | Maximize variance in fewer dimensions       | Keep only relevant features           |\n","| **Interpretability** | Low (components are abstract combinations)  | High (original features are retained) |\n","| **Example**          | PC1 = 0.6√óX1 + 0.8√óX2                       | Keep only X1 and X3, drop X2          |\n"],"metadata":{"id":"3GBOnkiLb44V"}},{"cell_type":"markdown","source":["Question 4: What are eigenvalues and eigenvectors in PCA, and why are they\n","important?\n","-- In PCA, eigenvalues and eigenvectors come from linear algebra, specifically from the covariance matrix of the data.\n","\n","They are core components used to:\n","\n","Identify principal components (directions of maximum variance).\n","\n","Determine how much variance each principal component captures.\n","\n"," What is an eigenvector?\n","\n","An eigenvector is a direction (a vector) along which a linear transformation acts by stretching or compressing.\n","\n","In the context of PCA:\n","\n","An eigenvector represents a principal component direction ‚Äî a new axis in the feature space.\n","\n","It defines the orientation of the principal component.\n","\n"," What is an eigenvalue?\n","\n","An eigenvalue is a scalar that tells how much variance is along its corresponding eigenvector.\n","\n","In PCA:\n","\n","Larger eigenvalue ‚Üí more variance captured by that component.\n","\n","Eigenvalues help you decide how many components to keep.\n","\n"," PCA Step Involving Eigenvalues & Eigenvectors:\n","\n","Compute the covariance matrix of the standardized data.\n","\n","Compute eigenvectors and eigenvalues of this matrix.\n","\n","Sort eigenvectors by their eigenvalues in descending order.\n","\n","Select the top k eigenvectors ‚Üí these form the principal components.\n","\n","Project the data onto these eigenvectors"],"metadata":{"id":"Q03GN0Neb4rQ"}},{"cell_type":"markdown","source":["Question 5: How do KNN and PCA complement each other when applied in a single\n","pipeline?\n","-- PCA and KNN work well together because PCA helps address key weaknesses of KNN ‚Äî especially in high-dimensional spaces ‚Äî making KNN more accurate, faster, and less prone to overfitting.\n","\n","| PCA's Role in Pipeline                | Benefit for KNN                                                                                                                                  |\n","| ------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------ |\n","|  **Reduces Dimensionality**         | KNN struggles in high dimensions due to the **curse of dimensionality**. PCA reduces the number of features while preserving important patterns. |\n","|  **Removes Noise / Redundancy**     | KNN treats all features equally. PCA filters out **noisy or irrelevant combinations**, helping KNN focus on more meaningful distances.           |\n","|  **Improves Speed**                  | KNN is computationally expensive during prediction. Reducing dimensionality via PCA reduces the **number of distance calculations per point**.   |\n","|  **Transforms Correlated Features** | KNN doesn‚Äôt handle multicollinearity well. PCA creates **uncorrelated components**, making the distance metric more reliable.                    |\n","|  **Better Visualization**           | You can visualize the data (e.g., in 2D or 3D after PCA), which helps interpret KNN‚Äôs behavior or spot misclassifications.                       |\n","\n","Real-World Scenario:\n","\n","Suppose you have a dataset with 100 features.\n","\n","Many of them are noisy, redundant, or correlated.\n","\n","PCA reduces it to, say, 10 informative components.\n","\n","KNN then operates in a cleaner, lower-dimensional space, improving performance.\n","\n"," Caution:\n","\n","PCA is unsupervised ‚Äî it does not consider class labels when reducing dimensions.\n","\n","So, it's possible that some important discriminative information may be lost.\n","\n","Use cross-validation to find the right number of PCA components before applying KNN.\n"],"metadata":{"id":"_xGmxUyHb4d6"}},{"cell_type":"code","source":["#Dataset:\n","#Use the Wine Dataset from sklearn.datasets.load_wine().\n","#uestion 6: Train a KNN Classifier on the Wine dataset with and without feature\n","#     caling. Compare model accuracy in both cases.\n","\n","from sklearn.datasets import load_wine\n","from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import accuracy_score\n","\n","# Load the wine dataset\n","data = load_wine()\n","X, y = data.data, data.target\n","\n","# Split into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.3, random_state=42, stratify=y\n",")\n","\n","# ---- 1. KNN without feature scaling ----\n","knn_no_scaling = KNeighborsClassifier(n_neighbors=5)\n","knn_no_scaling.fit(X_train, y_train)\n","y_pred_no_scaling = knn_no_scaling.predict(X_test)\n","accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n","\n","# ---- 2. KNN with feature scaling ----\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","knn_with_scaling = KNeighborsClassifier(n_neighbors=5)\n","knn_with_scaling.fit(X_train_scaled, y_train)\n","y_pred_with_scaling = knn_with_scaling.predict(X_test_scaled)\n","accuracy_with_scaling = accuracy_score(y_test, y_pred_with_scaling)\n","\n","print(f\"Accuracy without scaling: {accuracy_no_scaling:.4f}\")\n","print(f\"Accuracy with scaling: {accuracy_with_scaling:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GpCgyiLNepSR","executionInfo":{"status":"ok","timestamp":1756918211003,"user_tz":-330,"elapsed":2744,"user":{"displayName":"Saransh Soni","userId":"02741440045839695582"}},"outputId":"ffb1bc54-c146-46b7-bb5b-96d332f0b698"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy without scaling: 0.7222\n","Accuracy with scaling: 0.9444\n"]}]},{"cell_type":"code","source":["#Question 7: Train a PCA model on the Wine dataset and print the explained variance\n","# ratio of each principal component.\n","\n","from sklearn.datasets import load_wine\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA\n","import numpy as np\n","\n","# Load the wine dataset\n","data = load_wine()\n","X = data.data\n","\n","# Standardize the features (important for PCA!)\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# Apply PCA\n","pca = PCA()\n","X_pca = pca.fit_transform(X_scaled)\n","\n","# Print explained variance ratio\n","explained_variance = pca.explained_variance_ratio_\n","\n","# Display each component's explained variance\n","for i, variance in enumerate(explained_variance):\n","    print(f\"Principal Component {i+1}: {variance:.4f}\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H0Fn_H8_epFb","executionInfo":{"status":"ok","timestamp":1756918268674,"user_tz":-330,"elapsed":34,"user":{"displayName":"Saransh Soni","userId":"02741440045839695582"}},"outputId":"31912a47-6a89-41f4-c8fb-18272ea2cd6c"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Principal Component 1: 0.3620\n","Principal Component 2: 0.1921\n","Principal Component 3: 0.1112\n","Principal Component 4: 0.0707\n","Principal Component 5: 0.0656\n","Principal Component 6: 0.0494\n","Principal Component 7: 0.0424\n","Principal Component 8: 0.0268\n","Principal Component 9: 0.0222\n","Principal Component 10: 0.0193\n","Principal Component 11: 0.0174\n","Principal Component 12: 0.0130\n","Principal Component 13: 0.0080\n"]}]},{"cell_type":"code","source":["#Question 8: Train a KNN Classifier on the PCA-transformed dataset (retain top 2\n","# components). Compare the accuracy with the original dataset.\n","\n","from sklearn.datasets import load_wine\n","from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA\n"],"metadata":{"id":"OZYnBrD_eoz4","executionInfo":{"status":"ok","timestamp":1756918306654,"user_tz":-330,"elapsed":24,"user":{"displayName":"Saransh Soni","userId":"02741440045839695582"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Question 9: Train a KNN Classifier with different distance metrics (euclidean,\n","# manhattan) on the scaled Wine dataset and compare the results.\n","\n","from sklearn.datasets import load_wine\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import accuracy_score\n","\n","# Load the dataset\n","data = load_wine()\n","X, y = data.data, data.target\n","\n","# Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.3, random_state=42, stratify=y\n",")\n","\n","# Feature scaling (important for distance-based models)\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# ---- KNN with Euclidean distance (default) ----\n","knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n","knn_euclidean.fit(X_train_scaled, y_train)\n","y_pred_euclidean = knn_euclidean.predict(X_test_scaled)\n","acc_euclidean = accuracy_score(y_test, y_pred_euclidean)\n","\n","# ---- KNN with Manhattan distance ----\n","knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n","knn_manhattan.fit(X_train_scaled, y_train)\n","y_pred_manhattan = knn_manhattan.predict(X_test_scaled)\n","acc_manhattan = accuracy_score(y_test, y_pred_manhattan)\n","\n","# Print results\n","print(f\"Accuracy with Euclidean distance: {acc_euclidean:.4f}\")\n","print(f\"Accuracy with Manhattan distance: {acc_manhattan:.4f}\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0FvlF1c_em7n","executionInfo":{"status":"ok","timestamp":1756918367903,"user_tz":-330,"elapsed":22,"user":{"displayName":"Saransh Soni","userId":"02741440045839695582"}},"outputId":"ed48227e-ac5a-48ca-92e2-5ba59d46a993"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy with Euclidean distance: 0.9444\n","Accuracy with Manhattan distance: 0.9815\n"]}]},{"cell_type":"code","source":["# Question 10: You are working with a high-dimensional gene expression dataset to\n","#classify patients with different types of cancer.\n","#Due to the large number of features and a small number of samples, traditional models\n","#overfit.\n","#Explain how you would:\n","#‚óè Use PCA to reduce dimensionality\n","#‚óè Decide how many components to keep\n","#‚óè Use KNN for classification post-dimensionality reduction\n","#‚óè Evaluate the model\n","#‚óè Justify this pipeline to your stakeholders as a robust solution for real-world\n","#biomedical data\n","\n","from sklearn.decomposition import PCA\n","\n","# Standardize data first (very important for PCA)\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# Apply PCA\n","pca = PCA()\n","X_pca = pca.fit_transform(X_scaled)\n","\n"],"metadata":{"id":"Y7ePKgVTfttt","executionInfo":{"status":"ok","timestamp":1756918526691,"user_tz":-330,"elapsed":23,"user":{"displayName":"Saransh Soni","userId":"02741440045839695582"}}},"execution_count":5,"outputs":[]}]}